{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "substantial-prevention",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConvGRU import ConvGRU, ConvGRUCell\n",
    "from reformer.reformer_enc_dec import ReformerEncDec\n",
    "from reformer.reformer_pytorch import Reformer, ReformerLM\n",
    "from patchify import patchify, unpatchify\n",
    "from axial_positional_embedding import AxialPositionalEmbedding\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import cv2 as cv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from cv2 import VideoWriter, VideoWriter_fourcc, imread\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import warnings\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "dataset_dir = r\"C:\\Users/Leo's PC/Documents/SSTP Tests/SSTP/GruGan/test_frames\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "silver-treasurer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReformerDataset(Dataset):\n",
    "\n",
    "    def __init__(self, file_dir, transform=None, seq_len=1):\n",
    "\n",
    "        self.dir = file_dir\n",
    "        self.transform = transform\n",
    "        self.seq_len = seq_len\n",
    "        self.diction = [] # yes, yes, it is an array called diction\n",
    "\n",
    "        idx = 0\n",
    "        for filename in os.listdir(self.dir):\n",
    "            if filename.endswith('jpg'):\n",
    "                self.diction.append(filename)\n",
    "                idx += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.diction) - 1\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = time.time()\n",
    "        readImage = lambda filename: self.transform(np.array(cv.imread(os.path.join(self.dir, filename)) / 255)) if self.transform else np.array(cv.imread(os.path.join(self.dir, filename)) / 255)\n",
    "        \n",
    "        x, y = self.diction[idx*self.seq_len : (idx+1)*self.seq_len], self.diction[idx*self.seq_len+1 : (idx+1)*self.seq_len+1]\n",
    "        x, y = torch.Tensor(np.asarray(list(map(readImage, x)))), torch.Tensor(np.asarray(list(map(readImage, y))))\n",
    "        return [x, y]\n",
    "\n",
    "\n",
    "def HWC2CHW(x):\n",
    "    return np.array(x).transpose(2, 0, 1)\n",
    "\n",
    "\n",
    "dataset = ReformerDataset(file_dir=dataset_dir, transform=HWC2CHW, seq_len=256)\n",
    "\n",
    "loader = DataLoader(dataset=dataset, batch_size=4, shuffle=False, drop_last=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "union-riding",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dim, depth=6, heads=8, max_seq_len=16384, bucket_size=64):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.heads = heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.bucket_size = bucket_size\n",
    "\n",
    "        self.decoder = ReformerLM(\n",
    "            dim = self.dim,\n",
    "            depth = self.depth,\n",
    "            heads = self.heads,\n",
    "            max_seq_len = self.max_seq_len, # ~10 seconds\n",
    "            bucket_size = self.bucket_size,\n",
    "            causal = True,\n",
    "            embed = False,\n",
    "            return_embeddings = True #return the output of the last attention layer, the keys; otherwise would get a softmax activation of vocab dict distribution\n",
    "        ).cuda()\n",
    "        \n",
    "        self.pos_embedder = AxialPositionalEmbedding(256, (256, 64))\n",
    "        self.fmap_embedder = AxialPositionalEmbedding(256, (256, 64))\n",
    "    \n",
    "    #@autocast()\n",
    "    def forward(self, x):\n",
    "        \n",
    "        self.out = x + self.pos_embedder(x)\n",
    "        \n",
    "        #Positional Embedding\n",
    "        for b in range(len(self.out)): #batch\n",
    "            for i in range(int(len(self.out[b])/64)): #vector embeddings in a batch\n",
    "                self.out[b][i*64:(i+1)*64] = self.fmap_embedder(self.out[b][i*64:(i+1)*64].unsqueeze(0)).squeeze(0)\n",
    "                \n",
    "        self.out = self.decoder(self.out)\n",
    "\n",
    "        return self.out\n",
    "    \n",
    "\n",
    "class Input_Conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Input_Conv, self).__init__()\n",
    "        \n",
    "        # Initialize the DenseBlock, input shape is (n, 3, 256, 256), output shape is (n, 64, 16, 16)\n",
    "        self.denseblock = torchvision.models.densenet121()\n",
    "        self.denseblock.features.transition1.conv = nn.Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        self.denseblock.features.transition1.pool = nn.AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
    "        self.denseblock = nn.Sequential(*list(self.denseblock.features.children())[:6])\n",
    "    \n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        return self.denseblock(x)\n",
    "    \n",
    "\n",
    "class Output_ConvTranspose(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Output_ConvTranspose, self).__init__()\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "        \n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=[5,5], stride=1, padding=1)  \n",
    "        self.conv2 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=[5,5], stride=1, padding=1)  \n",
    "        self.conv3 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=[5,5], stride=1, padding=1)  \n",
    "        self.conv4 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=[5,5], stride=1, padding=1)\n",
    "        self.conv5 = nn.ConvTranspose2d(in_channels=64, out_channels=3, kernel_size=[1,1], stride=1, padding=0)\n",
    "    \n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        # input size (1, 64, 16, 16)\n",
    "        \n",
    "        self.out = self.conv1(x)\n",
    "        self.out = self.relu(self.out)  \n",
    "        self.out = self.upsample(self.out)\n",
    "        \n",
    "        self.out = self.conv2(self.out)\n",
    "        self.out = self.relu(self.out)  \n",
    "        self.out = self.upsample(self.out)\n",
    "        \n",
    "        self.out = self.conv3(self.out)\n",
    "        self.out = self.relu(self.out)  \n",
    "        self.out = self.upsample(self.out)\n",
    "        \n",
    "        self.out = self.conv4(self.out)\n",
    "        self.out = self.relu(self.out)  \n",
    "        self.out = self.upsample(self.out)\n",
    "        \n",
    "        self.out = self.conv5(self.out)\n",
    "        self.out = self.sigmoid(self.out)\n",
    "        \n",
    "        return self.out\n",
    "        \n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, dim, depth=6, heads=8, max_seq_len=16384, bucket_size=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.heads = heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.bucket_size = bucket_size\n",
    "        \n",
    "        self.inputconv = Input_Conv()\n",
    "        self.reformer = Decoder(dim=self.dim, depth=self.depth, heads=self.heads, max_seq_len=self.max_seq_len, bucket_size=self.bucket_size)\n",
    "        self.outputconvtranspose = Output_ConvTranspose()\n",
    "    \n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        #input shape is (b, n, c, h, w)\n",
    "        self.out = []\n",
    "        for b in x:\n",
    "            for n in b:\n",
    "                self.out.append(self.inputconv(n.unsqueeze(0)).cpu().detach().numpy())\n",
    "        self.out = torch.Tensor(self.out)\n",
    "        \n",
    "        self.unflattened_shape = self.out.shape\n",
    "        self.out = self.out.view(x.shape[0], self.max_seq_len, self.dim) #TODO padding for variable sequence length input\n",
    "        \n",
    "        self.out = self.reformer(self.out)\n",
    "        self.out = self.out.view(self.unflattened_shape)\n",
    "        \n",
    "        self.out = []\n",
    "        for b in self.out:\n",
    "            for n in b:\n",
    "                self.out.append(self.outputconvtranspose(n.unsqueeze(0)))\n",
    "        self.out = torch.Tensor(self.out)\n",
    "        \n",
    "        return self.out       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "assisted-sherman",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(dim=256).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-conjunction",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, imgs in enumerate(loader):\n",
    "    inp = Variable(imgs[0]).cuda()\n",
    "    with autocast():\n",
    "        out = G(inp)\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-illinois",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
