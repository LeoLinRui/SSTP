{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "reported-chance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConvGRU import ConvGRU, ConvGRUCell\n",
    "from reformer.reformer_enc_dec import ReformerEncDec\n",
    "from reformer.reformer_pytorch import Reformer, ReformerLM\n",
    "from patchify import patchify, unpatchify\n",
    "from axial_positional_embedding import AxialPositionalEmbedding\n",
    "from transformers import ReformerModel, ReformerConfig, ReformerTokenizer\n",
    "import deepspeed\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import cv2 as cv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "from cv2 import VideoWriter, VideoWriter_fourcc, imread\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import warnings\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "dataset_dir = r\"C:\\Users/Leo's PC/Documents/SSTP Tests/SSTP/GruGan/test_frames\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pediatric-collector",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img2embedding(imgs: np.array, patch_shape):\n",
    "    # N, C, H, W\n",
    "    out = []\n",
    "    if len(imgs.shape) == 3:\n",
    "        imgs = np.expand_dims(imgs, 1)\n",
    "    if imgs.shape[1] == 1: # if grayscale\n",
    "        for img in imgs:\n",
    "            img = img[0]\n",
    "            patches = patchify(img, patch_shape, step=patch_shape)\n",
    "            patches = np.reshape(patches, (int((img.shape[0]/patch_shape[0]) * (img.shape[1]/patch_shape[1])), int(patch_shape[0] * patch_shape[1])))\n",
    "            out.append(patches)\n",
    "            \n",
    "        out = np.asarray(out)\n",
    "        return out\n",
    "    \n",
    "toVTensor = lambda x : Variable(torch.Tensor(x).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "separated-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReformerDatasetFast(Dataset):\n",
    "\n",
    "    def __init__(self, file_dir, transform=None, seq_len=1):\n",
    "\n",
    "        self.dir = file_dir\n",
    "        self.transform = transform\n",
    "        self.seq_len = seq_len\n",
    "        self.diction = [] # yes, yes, it is an array called diction\n",
    "        \n",
    "        readImage = lambda filename: self.transform(np.array(cv.imread(os.path.join(self.dir, filename)) / 255)) if self.transform else np.array(cv.imread(os.path.join(self.dir, filename)) / 255)\n",
    "        \n",
    "        idx = 0\n",
    "        for filename in os.listdir(self.dir):\n",
    "            if filename.endswith('jpg'):\n",
    "                self.diction.append(readImage(filename))\n",
    "                idx += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.diction) - 1\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = time.time()\n",
    "        x, y = self.diction[idx*self.seq_len : (idx+1)*self.seq_len], self.diction[idx*self.seq_len+1 : (idx+1)*self.seq_len+1]\n",
    "        print(time.time() - start)\n",
    "        x, y = torch.Tensor(np.asarray(x)), torch.Tensor(np.asarray(y))\n",
    "        print(time.time() - start)\n",
    "        return [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "anonymous-cardiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReformerDataset(Dataset):\n",
    "\n",
    "    def __init__(self, file_dir, transform=None, seq_len=1):\n",
    "\n",
    "        self.dir = file_dir\n",
    "        self.transform = transform\n",
    "        self.seq_len = seq_len\n",
    "        self.diction = [] # yes, yes, it is an array called diction\n",
    "\n",
    "        idx = 0\n",
    "        for filename in os.listdir(self.dir):\n",
    "            if filename.endswith('jpg'):\n",
    "                self.diction.append(filename)\n",
    "                idx += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.diction) - 1\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = time.time()\n",
    "        readImage = lambda filename: self.transform(np.array(cv.imread(os.path.join(self.dir, filename)) / 255)) if self.transform else np.array(cv.imread(os.path.join(self.dir, filename)) / 255)\n",
    "        \n",
    "        x, y = self.diction[idx*self.seq_len : (idx+1)*self.seq_len], self.diction[idx*self.seq_len+1 : (idx+1)*self.seq_len+1]\n",
    "        x, y = torch.Tensor(np.asarray(list(map(readImage, x)))), torch.Tensor(np.asarray(list(map(readImage, y))))\n",
    "        return [x, y]\n",
    "\n",
    "\n",
    "def HWC2CHW(x):\n",
    "    return np.array(x).transpose(2, 0, 1)\n",
    "\n",
    "\n",
    "dataset = ReformerDataset(file_dir=dataset_dir, transform=HWC2CHW, seq_len=256)\n",
    "\n",
    "loader = DataLoader(dataset=dataset, batch_size=1, shuffle=False, drop_last=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "hungry-travel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.6571147441864014\n",
      "0.0\n",
      "0.7066318988800049\n",
      "0.0004951953887939453\n",
      "0.7661302089691162\n",
      "0.0\n",
      "0.7226262092590332\n",
      "torch.Size([4, 256, 3, 256, 256])\n",
      "torch.Size([4, 256, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for i, imgs in enumerate(loader):\n",
    "    for j in imgs:\n",
    "        print(j.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "copyrighted-spare",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder = ReformerLM(\n",
    "            dim = 256,\n",
    "            depth = 6,\n",
    "            heads = 8,\n",
    "            max_seq_len = 256,\n",
    "            bucket_size = 64,\n",
    "            causal = False,\n",
    "            embed = False,\n",
    "            return_embeddings = True #return the output of the last attention layer, the keys.\n",
    "        ).cuda()\n",
    "\n",
    "        self.decoder = ReformerLM(\n",
    "            dim = 256,\n",
    "            depth = 6,\n",
    "            heads = 8,\n",
    "            max_seq_len = 256,\n",
    "            bucket_size = 64,\n",
    "            causal = False,\n",
    "            embed = False,\n",
    "            return_embeddings = True #return the output of the last attention layer, the keys; otherwise would get a softmax activation of vocab dict distribution\n",
    "        ).cuda()\n",
    "        \n",
    "    def forward(self, x, y_prev):\n",
    "        self.encoded_keys = self.encoder(x)\n",
    "        self.output = self.decoder(y_prev, keys = self.encoded_keys)\n",
    "        return self.output\n",
    "    \n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.encoder = ReformerLM(\n",
    "            dim = 256,\n",
    "            depth = 6,\n",
    "            heads = 8,\n",
    "            max_seq_len = 256,\n",
    "            bucket_size = 64,\n",
    "            causal = False,\n",
    "            embed = False,\n",
    "            return_embeddings = True #return the output of the last attention layer, the keys.\n",
    "        ).cuda()\n",
    "        \n",
    "        self.decoder = ReformerLM(\n",
    "            dim = 256,\n",
    "            depth = 6,\n",
    "            heads = 8,\n",
    "            max_seq_len = 256,\n",
    "            bucket_size = 64,\n",
    "            causal = False,\n",
    "            embed = False,\n",
    "            return_embeddings = True #return the output of the last attention layer, the keys; otherwise would get a softmax activation of vocab dict distribution\n",
    "        ).cuda()\n",
    "        \n",
    "        self.fc = nn.Linear(256, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(x, y_prev):\n",
    "        self.encoded_keys = self.encoder(x)\n",
    "        self.embeddings = self.decoder(y_prev, keys = self.encoded_keys)\n",
    "        self.output = self.fc(self.embeddings)\n",
    "        self.output = self.sigmoid(self.output)\n",
    "        return self.output, self.embeddings\n",
    "            \n",
    "\n",
    "class Decoder_Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder_Generator, self).__init__()\n",
    "\n",
    "        self.decoder = ReformerLM(\n",
    "            dim = 256,\n",
    "            depth = 6,\n",
    "            heads = 8,\n",
    "            max_seq_len = 16384, # ~10 seconds\n",
    "            bucket_size = 64,\n",
    "            causal = True,\n",
    "            embed = False,\n",
    "            return_embeddings = True #return the output of the last attention layer, the keys; otherwise would get a softmax activation of vocab dict distribution\n",
    "        ).cuda()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.output = self.decoder(x)\n",
    "        return self.output\n",
    "    \n",
    "\n",
    "class Input_Conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Input_Conv, self).__init__()\n",
    "        \n",
    "        # Initialize the DenseBlock, input shape is (n, 3, 256, 256), output shape is (n, 64, 16, 16)\n",
    "        self.denseblock = torchvision.models.densenet121()\n",
    "        self.denseblock.features.transition1.conv = nn.Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        self.denseblock.features.transition1.pool = nn.AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
    "        self.denseblock = nn.Sequential(*list(self.denseblock.features.children())[:6])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.denseblock(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-compression",
   "metadata": {},
   "source": [
    "## Embeddings:\n",
    "### Generator:\n",
    "#### Encoder Input:\n",
    "Encoder input is the sample image, broken up into 16x16 patches (for a 256x256 image, that's 256 vectors of length 256). However, for a colored image (3x256x256,) the channels need to be mapped. To get the input, we first cat(R, G, B), which has shape (768, 256). But we need to concatenate RGB embeddings, as well as positional embeddings, to the end of each pixel array. If RGB encoding occupies 64 numbers and positional (in-image spatial) occupies 196 numbers, we'll get a vector of length 512 for each patch, making our input of shape (256, 512) for a 256x256 image.\n",
    "#### Decoder Input:\n",
    "Compare to the encoder input, the decoder needs to handle one more dimension -- the time dimension of the video frame sequence. \n",
    "### Discriminator:\n",
    "#### Encoder Input:\n",
    "(same as Generator encoder input) Encoder input is the sample image, broken up into 16x16 patches (for a 256x256 image, that's 256 vectors of length 256). However, for a colored image (3x256x256,) the channels need to be mapped. To get the input, we first cat(R, G, B), which has shape (768, 256). But we need to concatenate RGB embeddings, as well as positional embeddings, to the end of each pixel array. If RGB encoding occupies 64 numbers and positional (in-image spatial) occupies 196 numbers, we'll get a vector of length 512 for each patch, making our input of shape (256, 512) for a 256x256 image.\n",
    "#### Decoder Input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "affiliated-glenn",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuremap_embedder = nn.Embedding(num_embeddings=64, embedding_dim=128).cuda()\n",
    "sequence_position_embedder = nn.Embedding(num_embeddings=256, embedding_dim=128).cuda()\n",
    "\n",
    "CNN = Input_Conv().cuda()\n",
    "Decoder = Decoder_Generator().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "mighty-february",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2355,  1.0560,  1.5756, -0.1583, -1.6614,  0.1206, -0.7047,  0.5283,\n",
      "         -2.0649, -0.5863, -0.3184,  0.3223, -1.8305,  0.5860, -1.9457,  0.9726,\n",
      "         -0.2147,  1.5721,  1.0855, -0.7001,  0.7131,  1.4095,  1.2315,  0.6028,\n",
      "         -1.1094, -1.7353,  2.4519, -0.6465, -2.7521,  0.4146, -0.2117, -0.5878,\n",
      "          0.2334, -0.0936,  0.9556, -0.0413, -0.3043, -0.4374, -0.3279, -0.8669,\n",
      "         -0.0206, -0.3320, -0.3252, -0.2264,  1.4192,  0.0129,  0.5412, -0.0563,\n",
      "          1.3217, -0.5745,  0.4835, -0.4406,  0.8775,  0.9977, -0.1232,  0.3732,\n",
      "          0.9167, -1.7157,  1.4234,  1.9801, -0.1875,  1.2845, -0.9090, -0.2151,\n",
      "          0.7727,  0.3833,  0.0052,  0.4972, -0.2087, -0.2124,  2.0634,  1.5586,\n",
      "          0.8043, -0.0194,  0.2682, -1.2101,  0.2546,  0.0451, -0.6861, -1.2096,\n",
      "         -0.3958,  0.8871,  0.9399,  0.6088, -0.1564, -0.9575,  1.3501,  0.6570,\n",
      "         -0.2680,  1.3121, -0.7205,  0.0966, -2.1839,  0.0475,  0.4677,  2.1815,\n",
      "          0.0533, -0.8190,  0.1972,  0.0889, -1.7390, -0.6167, -0.4238, -0.7414,\n",
      "         -0.7441, -0.9404,  0.0689, -0.6339,  1.7569, -0.3637, -0.7359,  1.0139,\n",
      "         -0.1106, -1.2875,  1.1249,  0.5528,  0.3949,  0.1549,  0.3592,  1.6320,\n",
      "         -0.5137,  0.3508,  0.8817,  0.4857,  0.5161, -1.3844, -0.2396, -0.7263]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(sequence_position_embedder(torch.Tensor([0]).long().cuda()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "educated-voice",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embedder = AxialPositionalEmbedding(256, (256, 64))\n",
    "fmap_embedder = AxialPositionalEmbedding(256, (256, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "nearby-scoop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 256])\n",
      "torch.Size([1, 128, 256])\n"
     ]
    }
   ],
   "source": [
    "inp = toVTensor(np.random.rand(2, 3, 256, 256))\n",
    "out = CNN(inp)\n",
    "out = out.view(1, 128, 256)\n",
    "out = out + pos_embedder(out) + fmap_embedder(out)\n",
    "\n",
    "print(out.shape)\n",
    "\n",
    "out = Decoder(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-surrey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 512])\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "inp = toVTensor(np.random.rand(1, 3, 256, 256))\n",
    "out = CNN(inp)\n",
    "final = []\n",
    "for n, image in enumerate(out):\n",
    "    result = []\n",
    "    for f, featuremap in enumerate(image):\n",
    "        featuremap = torch.cat((featuremap.reshape(256), featuremap_embedder(torch.Tensor([f]).long().squeeze(0).cuda()), sequence_position_embedder(torch.Tensor([n]).long().squeeze(0).cuda())), dim=0)\n",
    "        result.append(featuremap.detach().cpu().numpy()) ##TODO this cannot be detached here, need to figure out how to do without\n",
    "    final.append(result)\n",
    "\n",
    "final = torch.Tensor(np.asarray(final))\n",
    "print(final.shape)\n",
    "print(final[0][0].shape)\n",
    "\n",
    "out = Decoder(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "complicated-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = Decoder(toVTensor(np.random.rand(1, 128, 512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "chubby-amino",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 512])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "iraqi-producer",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsconfig={\n",
    "    \"train_batch_size\":4,\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"Adam\",\n",
    "        \"params\": {\n",
    "          \"lr\": 0.001,\n",
    "          \"betas\": [0.8, 0.999],\n",
    "          \"eps\": 1e-8,\n",
    "          \"weight_decay\": 3e-7\n",
    "    }\n",
    "  },\n",
    "    \n",
    "    \"fp16\": {\n",
    "        \"enabled\": True,\n",
    "        \"loss_scale\": 0,\n",
    "        \"initial_scale_power\": 32,\n",
    "        \"loss_scale_window\": 1000,\n",
    "        \"hysteresis\": 2,\n",
    "        \"min_loss_scale\": 1\n",
    "    },\n",
    "    \n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        \"allgather_partitions\": True,\n",
    "        \"allgather_bucket_size\": 5e8,\n",
    "        \"overlap_comm\": False,\n",
    "        \"reduce_scatter\": True,\n",
    "        \"reduce_bucket_size\": 5e8,\n",
    "        \"contiguous_gradients\" : False,\n",
    "        \"cpu_offload\": False,\n",
    "        \"cpu_offload_params\" : False,\n",
    "        \"cpu_offload_use_pin_memory\" : False,\n",
    "        \"stage3_max_live_parameters\" : 1e9,\n",
    "        \"stage3_max_reuse_distance\" : 1e9,\n",
    "        \"stage3_prefetch_bucket_size\" : 5e8,\n",
    "        \"stage3_param_persistence_threshold\" : 1e6,\n",
    "        \"sub_group_size\" : 1e12\n",
    "        },\n",
    "    \n",
    "    \"logging\":{\n",
    "        \"steps_per_print\":100,\n",
    "        \"wall_clock_breakdown\":True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-neighborhood",
   "metadata": {},
   "source": [
    "## With the lucid Reformer, crashes the kernal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-defendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dim, depth=6, heads=8, max_seq_len=16384, bucket_size=64):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.heads = heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.bucket_size = bucket_size\n",
    "\n",
    "        self.decoder = ReformerLM(\n",
    "            dim = self.dim,\n",
    "            depth = self.depth,\n",
    "            heads = self.heads,\n",
    "            max_seq_len = self.max_seq_len, # ~10 seconds\n",
    "            bucket_size = self.bucket_size,\n",
    "            causal = True,\n",
    "            embed = False,\n",
    "            return_embeddings = True #return the output of the last attention layer, the keys; otherwise would get a softmax activation of vocab dict distribution\n",
    "        ).cuda()\n",
    "        \n",
    "        self.pos_embedder = AxialPositionalEmbedding(256, (256, 64))\n",
    "        self.fmap_embedder = AxialPositionalEmbedding(256, (256, 64))\n",
    "    \n",
    "    #@autocast()\n",
    "    def forward(self, x):\n",
    "        \n",
    "        self.out = x + self.pos_embedder(x)\n",
    "        \n",
    "        #Positional Embedding\n",
    "        for b in range(len(self.out)): #batch\n",
    "            for i in range(int(len(self.out[b])/64)): #vector embeddings in a batch\n",
    "                self.out[b][i*64:(i+1)*64] = self.fmap_embedder(self.out[b][i*64:(i+1)*64].unsqueeze(0)).squeeze(0)\n",
    "                \n",
    "        self.out = self.decoder(x)\n",
    "\n",
    "        return self.out\n",
    "    \n",
    "\n",
    "class Input_Conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Input_Conv, self).__init__()\n",
    "        \n",
    "        # Initialize the DenseBlock, input shape is (n, 3, 256, 256), output shape is (n, 64, 16, 16)\n",
    "        self.denseblock = torchvision.models.densenet121()\n",
    "        self.denseblock.features.transition1.conv = nn.Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        self.denseblock.features.transition1.pool = nn.AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
    "        self.denseblock = nn.Sequential(*list(self.denseblock.features.children())[:6])\n",
    "    \n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        return self.denseblock(x)\n",
    "    \n",
    "\n",
    "class Output_ConvTranspose(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Output_ConvTranspose, self).__init__()\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "        \n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=[5,5], stride=1, padding=1)  \n",
    "        self.conv2 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=[5,5], stride=1, padding=1)  \n",
    "        self.conv3 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=[5,5], stride=1, padding=1)  \n",
    "        self.conv4 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=[5,5], stride=1, padding=1)\n",
    "        self.conv5 = nn.ConvTranspose2d(in_channels=64, out_channels=3, kernel_size=[1,1], stride=1, padding=0)\n",
    "    \n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        # input size (1, 64, 16, 16)\n",
    "        \n",
    "        self.out = self.conv1(x)\n",
    "        self.out = self.relu(self.out)  \n",
    "        self.out = self.upsample(self.out)\n",
    "        \n",
    "        self.out = self.conv2(self.out)\n",
    "        self.out = self.relu(self.out)  \n",
    "        self.out = self.upsample(self.out)\n",
    "        \n",
    "        self.out = self.conv3(self.out)\n",
    "        self.out = self.relu(self.out)  \n",
    "        self.out = self.upsample(self.out)\n",
    "        \n",
    "        self.out = self.conv4(self.out)\n",
    "        self.out = self.relu(self.out)  \n",
    "        self.out = self.upsample(self.out)\n",
    "        \n",
    "        self.out = self.conv5(self.out)\n",
    "        self.out = self.sigmoid(self.out)\n",
    "        \n",
    "        return self.out\n",
    "        \n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, dim, depth=6, heads=8, max_seq_len=16384, bucket_size=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.heads = heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.bucket_size = bucket_size\n",
    "        \n",
    "        self.inputconv = Input_Conv()\n",
    "        self.reformer = Decoder(dim=self.dim, depth=self.depth, heads=self.heads, max_seq_len=self.max_seq_len, bucket_size=self.bucket_size)\n",
    "        self.outputconvtranspose = Output_ConvTranspose()\n",
    "    \n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        #input shape is (b, n, c, h, w)\n",
    "        self.out = []\n",
    "        for b in x:\n",
    "            for n in b:\n",
    "                self.out.append(self.inputconv(n.unsqueeze(0)).cpu().detach().numpy())\n",
    "        self.out = torch.Tensor(self.out)\n",
    "        \n",
    "        self.unflattened_shape = self.out.shape\n",
    "        self.out = self.out.view(x.shape[0], self.max_seq_len, self.dim) #TODO padding for variable sequence length input\n",
    "        \n",
    "        self.out = self.reformer(self.out)\n",
    "        self.out = self.out.view(self.unflattened_shape)\n",
    "        \n",
    "        self.out = []\n",
    "        for b in self.out:\n",
    "            for n in b:\n",
    "                self.out.append(self.outputconvtranspose(n.unsqueeze(0)))\n",
    "        self.out = torch.Tensor(self.out)\n",
    "        \n",
    "        return self.out       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "approved-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using Huggingface's Reformer implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "criminal-engineering",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b3e7216de9f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Initializing a Reformer model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mReformerModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'google/reformer-crime-and-punishment'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mReformerTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'google/reformer-crime-and-punishment'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    960\u001b[0m                 \u001b[0muse_auth_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m                 \u001b[0mrevision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 962\u001b[1;33m                 \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    963\u001b[0m             )\n\u001b[0;32m    964\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \"\"\"\n\u001b[1;32m--> 376\u001b[1;33m         \u001b[0mconfig_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    422\u001b[0m                 \u001b[0mresume_download\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m                 \u001b[0mlocal_files_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m                 \u001b[0muse_auth_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m             )\n\u001b[0;32m    426\u001b[0m             \u001b[1;31m# Load config dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   1084\u001b[0m             \u001b[0muser_agent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1086\u001b[1;33m             \u001b[0mlocal_files_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1087\u001b[0m         )\n\u001b[0;32m   1088\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   1263\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m                     raise ValueError(\n\u001b[1;32m-> 1265\u001b[1;33m                         \u001b[1;34m\"Connection error, and we cannot find the requested files in the cached path.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1266\u001b[0m                         \u001b[1;34m\" Please try again or make sure your Internet connection is on.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m                     )\n",
      "\u001b[1;31mValueError\u001b[0m: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on."
     ]
    }
   ],
   "source": [
    "# Initializing a Reformer configuration\n",
    "configuration = ReformerConfig(attention_head_size=64, attn_layers=['local', 'lsh', 'local', 'lsh', 'local', 'lsh'], axial_norm_std=1.0, axial_pos_embds=True, axial_pos_shape=[64, 64], \n",
    "                               axial_pos_embds_dim=[64, 192], chunk_size_lm_head=0, eos_token_id=2, feed_forward_size=256, hash_seed=None, hidden_act='relu', hidden_dropout_prob=0.05, \n",
    "                               hidden_size=256, initializer_range=0.02, is_decoder=False, layer_norm_eps=1e-12, local_num_chunks_before=1, local_num_chunks_after=0, \n",
    "                               local_attention_probs_dropout_prob=0.05, local_attn_chunk_length=64, lsh_attn_chunk_length=64, lsh_attention_probs_dropout_prob=0.0, lsh_num_chunks_before=1, \n",
    "                               lsh_num_chunks_after=0, max_position_embeddings=4096, num_attention_heads=12, num_buckets=None, num_hashes=1, pad_token_id=0, vocab_size=320, tie_word_embeddings=False, \n",
    "                               use_cache=True)\n",
    "\n",
    "# Initializing a Reformer model\n",
    "model = ReformerModel.from_pretrained('google/reformer-crime-and-punishment')\n",
    "tokenizer = ReformerTokenizer.from_pretrained('google/reformer-crime-and-punishment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "neither-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dim, depth=6, heads=8, max_seq_len=16384, bucket_size=64):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.heads = heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.bucket_size = bucket_size\n",
    "\n",
    "        # Initializing a Reformer configuration\n",
    "        self.configuration = ReformerConfig(attention_head_size=64, attn_layers=['local', 'lsh', 'local', 'lsh', 'local', 'lsh'], axial_norm_std=1.0, axial_pos_embds=True, axial_pos_shape=[256, 64], \n",
    "                                       axial_pos_embds_dim=[64, 192], chunk_size_lm_head=0, eos_token_id=2, feed_forward_size=256, hash_seed=None, hidden_act='relu', hidden_dropout_prob=0.05, \n",
    "                                       hidden_size=256, initializer_range=0.02, is_decoder=True, layer_norm_eps=1e-12, local_num_chunks_before=1, local_num_chunks_after=0, \n",
    "                                       local_attention_probs_dropout_prob=0.05, local_attn_chunk_length=64, lsh_attn_chunk_length=64, lsh_attention_probs_dropout_prob=0.0, lsh_num_chunks_before=1, \n",
    "                                       lsh_num_chunks_after=0, max_position_embeddings=16384, num_attention_heads=self.heads, num_buckets=None, num_hashes=1, pad_token_id=0, vocab_size=320, \n",
    "                                       tie_word_embeddings=False, use_cache=False, target_mapping=None)\n",
    "\n",
    "        # Initializing a Reformer model\n",
    "        self.decoder = ReformerModel(self.configuration)\n",
    "        \n",
    "        # self.pos_embedder = AxialPositionalEmbedding(256, (256, 64))\n",
    "        self.fmap_embedder = AxialPositionalEmbedding(256, (256, 64))\n",
    "    \n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # self.out = x + self.pos_embedder(x)\n",
    "        self.out = x\n",
    "        \n",
    "        #Positional Embedding\n",
    "        for b in range(len(self.out)): #batch\n",
    "            for i in range(int(len(self.out[b])/64)): #vector embeddings in a batch\n",
    "                self.out[b][i*64:(i+1)*64] = self.fmap_embedder(self.out[b][i*64:(i+1)*64].unsqueeze(0)).squeeze(0)\n",
    "        \n",
    "        print(self.out.shape)\n",
    "        self.out = self.decoder(inputs_embeds=self.out)\n",
    "\n",
    "        return self.out.last_hidden_state\n",
    "    \n",
    "\n",
    "class Input_Conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Input_Conv, self).__init__()\n",
    "        \n",
    "        # Initialize the DenseBlock, input shape is (n, 3, 256, 256), output shape is (n, 64, 16, 16)\n",
    "        self.denseblock = torchvision.models.densenet121()\n",
    "        self.denseblock.features.transition1.conv = nn.Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        self.denseblock.features.transition1.pool = nn.AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
    "        self.denseblock = nn.Sequential(*list(self.denseblock.features.children())[:6])\n",
    "    \n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        return self.denseblock(x)\n",
    "    \n",
    "\n",
    "class Output_ConvTranspose(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Output_ConvTranspose, self).__init__()\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "        \n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=[3,3], stride=1, padding=1)  \n",
    "        self.conv2 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=[3,3], stride=1, padding=1)  \n",
    "        self.conv3 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=[3,3], stride=1, padding=1)  \n",
    "        self.conv4 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=[3,3], stride=1, padding=1)\n",
    "        self.conv5 = nn.ConvTranspose2d(in_channels=64, out_channels=3, kernel_size=[1,1], stride=1, padding=0)\n",
    "    \n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        # input size (1, 64, 16, 16)\n",
    "        \n",
    "        self.out = self.conv1(x)\n",
    "        self.out = self.relu(self.out)  \n",
    "        self.out = self.upsample(self.out)\n",
    "        \n",
    "        self.out = self.conv2(self.out)\n",
    "        self.out = self.relu(self.out)  \n",
    "        self.out = self.upsample(self.out)\n",
    "        \n",
    "        self.out = self.conv3(self.out)\n",
    "        self.out = self.relu(self.out)  \n",
    "        self.out = self.upsample(self.out)\n",
    "        \n",
    "        self.out = self.conv4(self.out)\n",
    "        self.out = self.relu(self.out)  \n",
    "        self.out = self.upsample(self.out)\n",
    "        \n",
    "        self.out = self.conv5(self.out)\n",
    "        self.out = self.sigmoid(self.out)\n",
    "        \n",
    "        return self.out\n",
    "        \n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, dim, depth=6, heads=8, max_seq_len=16384, bucket_size=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.heads = heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.bucket_size = bucket_size\n",
    "        \n",
    "        self.inputconv = Input_Conv()\n",
    "        self.reformer = Decoder(dim=self.dim, depth=self.depth, heads=self.heads, max_seq_len=self.max_seq_len, bucket_size=self.bucket_size)\n",
    "        self.outputconvtranspose = Output_ConvTranspose()\n",
    "    \n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        #input shape is (b, n, c, h, w)\n",
    "        self.out = []\n",
    "        for b in x:\n",
    "            for n in b:\n",
    "                self.out.append(self.inputconv(n.unsqueeze(0)).squeeze(0).cpu().detach().numpy())\n",
    "        self.out = torch.Tensor(self.out).cuda()\n",
    "        \n",
    "        self.unflattened_shape = self.out.shape\n",
    "        self.out = self.out.view(x.shape[0], self.max_seq_len, self.dim) #TODO padding for variable sequence length input\n",
    "        \n",
    "        self.out = self.reformer(self.out)\n",
    "        print(self.out.shape)\n",
    "        self.out = self.out.view(1, 256, 128, 16, 16) #TODO this need to be changed for adaptive sizing\n",
    "        \n",
    "        self.outarray = []\n",
    "        for b in self.out:\n",
    "            for n in b:\n",
    "                self.outarray.append(self.outputconvtranspose(n.unsqueeze(0)).squeeze(0))\n",
    "        #self.out = torch.Tensor(self.outarray)\n",
    "        \n",
    "        return self.outarray     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "loving-outside",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(dim=256).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "extensive-category",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16384, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.num_buckets is not set. Setting config.num_buckets to [16, 32]...\n",
      "config.num_buckets is not set. Setting config.num_buckets to [16, 32]...\n",
      "config.num_buckets is not set. Setting config.num_buckets to [16, 32]...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16384, 512])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([1, 16384, 256])\n",
      "torch.Size([1, 16384, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 8.00 GiB total capacity; 5.83 GiB already allocated; 6.97 MiB free; 5.83 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-36497dd9ffb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\site-packages\\torch\\cuda\\amp\\autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-977c9f6f3d94>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputconvtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m         \u001b[1;31m#self.out = torch.Tensor(self.outarray)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\site-packages\\torch\\cuda\\amp\\autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-977c9f6f3d94>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupsample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1136\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1137\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 8.00 GiB total capacity; 5.83 GiB already allocated; 6.97 MiB free; 5.83 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "for i, imgs in enumerate(loader):\n",
    "    inp = Variable(imgs[0]).cuda()\n",
    "    with autocast():\n",
    "        out = G(inp)\n",
    "    print(out[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "formed-tucson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-03-15 20:07:26,167] [INFO] [logging.py:60:log_dist] [Rank -1] DeepSpeed info: version=0.3.12, git-hash=unknown, git-branch=unknown\n",
      "[2021-03-15 20:07:26,169] [INFO] [distributed.py:37:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['hostname -I']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c70bad23888f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#dsconfig = json.loads(dsconfig)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdensenet121\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel_engine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepspeed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdsconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_parameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\site-packages\\deepspeed\\__init__.py\u001b[0m in \u001b[0;36minitialize\u001b[1;34m(args, model, optimizer, model_parameters, training_data, lr_scheduler, mpu, dist_init_required, collate_fn, config_params)\u001b[0m\n\u001b[0;32m    119\u001b[0m                                  \u001b[0mdist_init_required\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdist_init_required\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m                                  \u001b[0mcollate_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m                                  config_params=config_params)\n\u001b[0m\u001b[0;32m    122\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mmpu\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"mpu must be None with pipeline parallelism\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\site-packages\\deepspeed\\runtime\\engine.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, model, optimizer, model_parameters, training_data, lr_scheduler, mpu, dist_init_required, collate_fn, config_params, dont_change_device)\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[1;31m# Initialize torch distributed if needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minit_distributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist_backend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdist_backend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0msee_memory_usage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"DeepSpeed Engine: Before args sanity test\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\site-packages\\deepspeed\\utils\\distributed.py\u001b[0m in \u001b[0;36minit_distributed\u001b[1;34m(dist_backend, auto_mpi_discovery, distributed_port, verbose, timeout, init_method)\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mpatch_aml_env_for_torch_nccl_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0mmpi_discovery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistributed_port\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdistributed_port\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\site-packages\\deepspeed\\utils\\distributed.py\u001b[0m in \u001b[0;36mmpi_discovery\u001b[1;34m(distributed_port, verbose)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mrank\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mhostname_cmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"hostname -I\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhostname_cmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[0mmaster_addr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mmaster_addr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaster_addr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[1;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[1;32m--> 389\u001b[1;33m                **kwargs).stdout\n\u001b[0m\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m             raise CalledProcessError(retcode, process.args,\n\u001b[1;32m--> 481\u001b[1;33m                                      output=stdout, stderr=stderr)\n\u001b[0m\u001b[0;32m    482\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mCompletedProcess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command '['hostname -I']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "#dsconfig = json.loads(dsconfig)\n",
    "model = torchvision.models.densenet121()\n",
    "model_engine, optimizer, _, _ = deepspeed.initialize(args=dsconfig, model=model, model_parameters=model.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-government",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
