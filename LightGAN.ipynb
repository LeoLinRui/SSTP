{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light GAN 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2 as cv\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100 # type=int, \"number of epochs of training\"\n",
    "batch_size = 10 # type=int, \"size of the batches\"\n",
    "\n",
    "lr = 0.0005 # type=float \"adam: learning rate\"\n",
    "b1 = 0.9 # type=float \"adam: decay of first order momentum of gradient\"\n",
    "b2 = 0.999 # type=float \"adam: decay of first order momentum of gradient\"\n",
    "\n",
    "num_gpu = 2 \n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "latent_dim = 4 # type=int \"dimensionality of the latent space\"\n",
    "img_size = 1024 # type=int \"size of each image dimension\"\n",
    "channels = 1 # type=int \"number of image channels\"\n",
    "sample_interval = 10000 # int \"interval betwen image samples\"\n",
    "\n",
    "dataset_dir = r\"C:\\Users\\Leo's PC\\Documents\\SSTP Tests\\stylegan2-ada-pytorch\\Font1024\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, file_dir, transform=None):\n",
    "\n",
    "        self.dir = file_dir\n",
    "        self.transform = transform\n",
    "        self.diction = {}\n",
    "        \n",
    "        idx = 0\n",
    "        for filename in os.listdir(self.dir):\n",
    "            if filename.endswith('png'):\n",
    "                self.diction[idx] = filename\n",
    "                idx += 1\n",
    "                        \n",
    "    def __len__(self):\n",
    "        return len(self.diction)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.diction[idx]\n",
    "        directory = self.dir + \"\\\\\" + str(img_name)\n",
    "        image = cv.imread(directory, cv.IMREAD_GRAYSCALE)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "    \n",
    "\n",
    "dataset = Dataset(file_dir=dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        #activation functions\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        #upsampler\n",
    "        self.upsamplerx4 = nn.Upsample(scale_factor=4)\n",
    "        self.upsamplerx2 = nn.Upsample(scale_factor=2)\n",
    "        self.pool = nn.AdaptiveMaxPool2d(output_size = 1024)\n",
    "        \n",
    "        #L1\n",
    "        self.conv1 = torch.nn.ConvTranspose2d(in_channels=512, out_channels=512, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.norm1 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        #L2\n",
    "        self.conv2 = torch.nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=5, stride=2, padding=2, bias=True)\n",
    "        self.norm2 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        #L3\n",
    "        self.conv3 = torch.nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=5, stride=2, padding=2, bias=True)\n",
    "        self.norm3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        #L4\n",
    "        self.conv4 = torch.nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=5, stride=2, padding=2, bias=True)\n",
    "        self.norm4 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        #L5\n",
    "        self.conv5 = torch.nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=7, stride=2, padding=2, bias=True)\n",
    "        self.norm5 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        #L6\n",
    "        self.conv6 = torch.nn.ConvTranspose2d(in_channels=32, out_channels=channels, kernel_size=7, stride=2, padding=1, bias=True)\n",
    "        self.norm6 = nn.BatchNorm2d(channels)\n",
    "\n",
    "\n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #L1\n",
    "        x = self.conv1(x)\n",
    "        x = self.upsamplerx2(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        \n",
    "        #L2\n",
    "        x = self.conv2(x)\n",
    "        x = self.upsamplerx2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        \n",
    "        #L3\n",
    "        x = self.conv3(x)\n",
    "        x = self.upsamplerx2(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        \n",
    "        #L4\n",
    "        x = self.conv4(x)\n",
    "        x = self.norm4(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        \n",
    "        #L5\n",
    "        x = self.conv5(x)\n",
    "        x = self.norm5(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        \n",
    "        #L6\n",
    "        x = self.conv6(x)\n",
    "        #x = self.pool(x)\n",
    "        x = self.norm6(x)\n",
    "        x = self.tanh(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def name(self):\n",
    "        return \"Generator\"\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        #activation functions\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        #L1\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=32, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.norm1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.AdaptiveMaxPool2d(output_size=512)\n",
    "        \n",
    "        #L2\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.norm2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.AdaptiveMaxPool2d(output_size=256)\n",
    "        \n",
    "        #L3\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.norm3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.AdaptiveMaxPool2d(output_size=128)\n",
    "        \n",
    "        #L4\n",
    "        self.conv4 = torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.norm4 = nn.BatchNorm2d(256)\n",
    "        self.pool4 = nn.AdaptiveMaxPool2d(output_size=64)\n",
    "        \n",
    "        #L5\n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.norm5 = nn.BatchNorm2d(512)\n",
    "        self.pool5 = nn.AdaptiveMaxPool2d(output_size = 32)\n",
    "        \n",
    "        #L6\n",
    "        self.conv6 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.norm6 = nn.BatchNorm2d(1024)\n",
    "        self.pool6 = nn.AdaptiveMaxPool2d(output_size=1)\n",
    "        \n",
    "        #L7\n",
    "        self.fc1 = nn.Linear(in_features=1024, out_features=512, bias=True)\n",
    "        self.norm7 = nn.BatchNorm1d(512)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        \n",
    "        #L8\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=2, bias=True)\n",
    "\n",
    "\n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        #L1\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        \n",
    "        #L2\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        \n",
    "        #L3\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        \n",
    "        #L4\n",
    "        x = self.conv4(x)\n",
    "        x = self.norm4(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        \n",
    "        #L5\n",
    "        x = self.conv5(x)\n",
    "        x = self.norm5(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x = self.pool5(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        \n",
    "        #L6\n",
    "        x = self.conv6(x)\n",
    "        x = self.norm6(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x = self.pool6(x)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "         \n",
    "        #print(x.shape)\n",
    "        \n",
    "        #L7\n",
    "        x = self.fc1(x)\n",
    "        x = self.norm7(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        \n",
    "        #L8\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def name(self):\n",
    "        return \"Discriminator\"\n",
    "    \n",
    "    \n",
    "class Discriminator_Res(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator_Res, self).__init__()\n",
    "        \n",
    "        self.prepool = nn.AdaptiveAveragePool2d(512, 512)\n",
    "        self.ResNet = torchvision.models.resnet18(pretrained=True)\n",
    "        self.ResNet.fc = nn.Linear(in_features=512, out_features=1, bias=True)\n",
    "\n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.ResNet(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def name(self):\n",
    "        return \"Discriminator_Res\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss, Optimizer, Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "G = Generator()\n",
    "D = Discriminator()\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d or type(m) == nn.ConvTranspose2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "G.apply(init_weights)\n",
    "D.apply(init_weights)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "G.cuda()\n",
    "D.cuda()\n",
    "adversarial_loss.cuda()\n",
    "\n",
    "\n",
    "G = torch.nn.DataParallel(G)\n",
    "D = torch.nn.DataParallel(D)\n",
    "\n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 2850/2851] [D loss: 0.724098] [G loss: 0.474077]\n",
      "[Epoch 1/100] [Batch 2850/2851] [D loss: 0.724077] [G loss: 0.474077]\n"
     ]
    }
   ],
   "source": [
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    for idx, imgs in enumerate(loader):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(imgs.shape[0], 2).fill_(1.0), requires_grad=False).cuda()\n",
    "        fake = Variable(Tensor(imgs.shape[0], 2).fill_(0.0), requires_grad=False).cuda()\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor)).cuda().to(device)\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        latent_vector = Variable(Tensor(np.random.randn(imgs.shape[0], 512, latent_dim, latent_dim))).cuda()\n",
    "        \n",
    "        G.train()\n",
    "        D.eval()\n",
    "        \n",
    "        with autocast():\n",
    "            gen_imgs = G(latent_vector) # Generate a batch of images\n",
    "            g_loss = adversarial_loss(D(gen_imgs), valid) # Loss measures generator's ability to fool the discriminator\n",
    "\n",
    "        scaler.scale(g_loss).backward() #back propagation with calculated loss\n",
    "        scaler.step(optimizer_G) \n",
    "        scaler.update()\n",
    "        \n",
    "        g_loss_avg = g_loss.item() if idx==0 else (g_loss_avg + g_loss.item()) / 2\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        \n",
    "        D.train()\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        real_imgs.unsqueeze_(1)\n",
    "        \n",
    "        with autocast():\n",
    "            # Measure discriminator's ability to classify real from generated samples\n",
    "            real_loss = adversarial_loss(D(real_imgs), valid)\n",
    "            fake_loss = adversarial_loss(D(gen_imgs.detach()), fake)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        scaler.scale(d_loss).backward() #back propagation with calculated loss\n",
    "        scaler.step(optimizer_D) \n",
    "        scaler.update()\n",
    "\n",
    "       \n",
    "        batches_done = epoch * len(loader) + idx\n",
    "        \n",
    "        d_loss_avg = d_loss.item() if idx==0 else (d_loss_avg + d_loss.item()) / 2\n",
    "        \n",
    "    save_image(gen_imgs.data[:25], r\"C:/Users/Leo's PC/Documents/SSTP Tests/Chinese Characters/LightGAN out/%d.png\" % batches_done, nrow=5, normalize=True)\n",
    "    print(\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\" % (epoch, n_epochs, idx, len(loader), d_loss_avg, g_loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint_file = open(r\"C:/Users/Leo's PC/Documents/SSTP Tests/Chinese Characters/LightGAN out/G.tar\", 'wb')\n",
    "torch.save({'model': G.state_dict()}, checkpoint_file)\n",
    "checkpoint_file.close()\n",
    "'''\n",
    "\n",
    "checkpoint = torch.load(open(\"C:/Users/Leo's PC/Documents/SSTP Tests/Chinese Characters/LightGAN out/G.tar\", 'rb'))\n",
    "G.load_state_dict(checkpoint['model'])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}