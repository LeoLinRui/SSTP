{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-translator",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from this StackOverflow answer: https://stackoverflow.com/a/39225039\n",
    "#and this Github repo: https://github.com/nsadawi/Download-Large-File-From-Google-Drive-Using-Python\n",
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in tqdm(\n",
    "            response.iter_content(1048576),\n",
    "            unit=\"MiB\", desc=f\"Downloading {destination}\"\n",
    "        ):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "                \n",
    "def download_multiple(files={}):\n",
    "    \"\"\"\n",
    "    Downloads Gdrive files from IDs into specified paths\n",
    "    Takes dictionary of {\"filepath\": \"ID\"}\n",
    "    \n",
    "    Note: \n",
    "    filepath must include filename, i.e. /directory/file.ext\n",
    "    filename does not need to match filename on Gdrive\n",
    "    \"\"\"\n",
    "    \n",
    "    for fp, ID in files.items():\n",
    "        dir_name = os.path.dirname(fp)\n",
    "        \n",
    "        if not os.path.exists(dir_name):\n",
    "            os.mkdir(dir_name)\n",
    "        \n",
    "        download_file_from_google_drive(ID, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#for testing\n",
    "files={ \"1558M/model.ckpt.meta\": \"10HlhfjbVNSuchYa2hmk9U1OlSRK6ZZxy\",\n",
    "        \"774M/model.ckpt.meta\": \"1-omUh72kLcFCKecKTn6RMduJh51dgnC-\",\n",
    "        \"out/shchedule.pdf\": \"1SZ76oFavmUeXeee1GElfpfwULzOxVUEH\"\n",
    "      }\n",
    "\"\"\"\n",
    "\n",
    "# Actual downloads\n",
    "files={ \"774M/model.ckpt.data-00000-of-00001\": \"1-iNTSsuloHKMzZA-ZDLSHN88RbWa3WrN\",\n",
    "        \"1558M/model.ckpt.data-00000-of-00001\": \"107pyhj1vKojyoiFS0GJTyo0AWZNntUyo\"\n",
    "      }\n",
    "\n",
    "download_multiple(files)"
   ]
  },
  {
   "source": [
    "# Twitter-stream download and processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download generic tweets\n",
    "from download_tools import download_files\n",
    "downloads = {\n",
    "    \"twitter_stream_2018_11_01.tar\": \"https://archive.org/download/archiveteam-twitter-stream-2018-11/twitter_stream_2018_11_01.tar\",\n",
    "    \"twitter_stream_2018_11_02.tar\": \"https://archive.org/download/archiveteam-twitter-stream-2018-11/twitter_stream_2018_11_02.tar\",\n",
    "    \"twitter_stream_2018_11_03.tar\": \"https://archive.org/download/archiveteam-twitter-stream-2018-11/twitter_stream_2018_11_03.tar\"\n",
    "    }\n",
    "location = \"D:/twitter-stream/twitter-stream-2018-11\"\n",
    "download_files(downloads, location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "from glob import glob\n",
    "\n",
    "work_dir = \"D:/twitter-stream/twitter-stream-2018-11/\"\n",
    "save_dir = \"D:/twitter-stream/twitter-stream-2018-11/\"\n",
    "\n",
    "for fp in glob(work_dir + \"**/*.tar\", recursive=True):\n",
    "    print(f\"Untarring {fp}\")\n",
    "    tarfile.open(fp).extractall(save_dir)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = input(\"work directory (source)\")\n",
    "save_dir = input(\"save directory (not file):\")\n",
    "\n",
    "dir_list = []\n",
    "\n",
    "for root, _, filenames in os.walk(work_dir):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\"bz2\"):\n",
    "            dir_list.append(os.path.join(root, filename))\n",
    "\n",
    "print(len(dir_list), \"files found\")\n",
    "\n",
    "\n",
    "tweet_list = p_map(process, dir_list, num_cpus=multiprocessing.cpu_count() - 1) # Leave 1 thread for other processes\n",
    "tweet_list = sum(tweet_list, [])\n",
    "\n",
    "\n",
    "print(len(tweet_list), 'Tweets gathered\\n\\n Samples:\\n')\n",
    "for tweet in tweet_list[:5]:\n",
    "    print(tweet)\n",
    "\n",
    "\n",
    "with open(os.path.join(save_dir, str(len(tweet_list)) + \"tweets.csv\"), 'w', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow(tweet_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3612jvsc74a57bd040003ae04e217409ff8e2df05e623ece41b99cdddf64ff392ef6b495500816d1",
   "display_name": "Python 3.6.12 64-bit ('Scraping': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}