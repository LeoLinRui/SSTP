{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "stopped-fourth",
   "metadata": {},
   "source": [
    "[Leo]\n",
    "\n",
    "The code help you to explore the basics of Dall-E and make sure it's working. \n",
    "\n",
    "First, install the lib via pip: pip install dalle-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-thickness",
   "metadata": {},
   "source": [
    "## DALL-E and VAE\n",
    "\n",
    "Initialize a variable auto encoder, pass it into the dall-E model to __init__.\n",
    "\n",
    "It will downloda the model at first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "blank-framing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████| 215185363/215185363 [00:31<00:00, 6750386.17it/s]\n",
      "100%|████████████████████████| 175360231/175360231 [00:24<00:00, 7199733.14it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dalle_pytorch import OpenAIDiscreteVAE, DALLE\n",
    "\n",
    "vae = OpenAIDiscreteVAE()       # loads pretrained OpenAI VAE\n",
    "\n",
    "dalle = DALLE(\n",
    "    dim = 1024,\n",
    "    vae = vae,                  # automatically infer (1) image sequence length and (2) number of image tokens\n",
    "    num_text_tokens = 10000,    # vocab size for text\n",
    "    text_seq_len = 256,         # text sequence length\n",
    "    depth = 1,                  # should aim to be 64\n",
    "    heads = 16,                 # attention heads\n",
    "    dim_head = 64,              # attention head dimension\n",
    "    attn_dropout = 0.1,         # attention dropout\n",
    "    ff_dropout = 0.1,           # feedforward dropout\n",
    "    reversible = False          # setting this to True will allow you to use a big network without memory costs, but a 2x computation cost\n",
    "    attn_types = ('full', 'axial_row', 'axial_col', 'conv_like')  # cycles between these four types of attention, can also pick one\n",
    ")\n",
    "\n",
    "text = torch.randint(0, 10000, (4, 256))\n",
    "images = torch.randn(4, 3, 256, 256)\n",
    "mask = torch.ones_like(text).bool()\n",
    "\n",
    "loss = dalle(text, images, mask = mask, return_loss = True)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-photographer",
   "metadata": {},
   "source": [
    "Above is all what you need to train a Dall-E model.\n",
    "\n",
    "Note that OpenAIDiscreteVAE is pre-trained, DiscreteVAE is not pretrained.\n",
    "\n",
    "Dall-E itself is not pre-trained. There's no offical weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-artist",
   "metadata": {},
   "source": [
    "## CLIP\n",
    "CLIP is what OpenAI uses to filter the output of DALL-E, only picking the good ones to present.\n",
    "\n",
    "CLIP needs to be passed into the DALL-E model at inference time.\n",
    "\n",
    "CLIP needs to be trained as well. There's no offical weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "powerful-twist",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dalle_pytorch import CLIP\n",
    "\n",
    "clip = CLIP(\n",
    "    dim_text = 512,\n",
    "    dim_image = 512,\n",
    "    dim_latent = 512,\n",
    "    num_text_tokens = 10000,\n",
    "    text_enc_depth = 6,\n",
    "    text_seq_len = 256,\n",
    "    text_heads = 8,\n",
    "    num_visual_tokens = 512,\n",
    "    visual_enc_depth = 6,\n",
    "    visual_image_size = 256,\n",
    "    visual_patch_size = 32,\n",
    "    visual_heads = 8\n",
    ")\n",
    "\n",
    "text = torch.randint(0, 10000, (4, 256))\n",
    "images = torch.randn(4, 3, 256, 256)\n",
    "mask = torch.ones_like(text).bool()\n",
    "\n",
    "loss = clip(text, images, text_mask = mask, return_loss = True)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-analysis",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
