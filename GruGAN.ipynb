{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gru GAN 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2 as cv\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100 # type=int, \"number of epochs of training\"\n",
    "batch_size = 8 # type=int, \"size of the batches\"\n",
    "\n",
    "lr = 0.002 # type=float \"adam: learning rate\"\n",
    "b1 = 0.9 # type=float \"adam: decay of first order momentum of gradient\"\n",
    "b2 = 0.999 # type=float \"adam: decay of first order momentum of gradient\"\n",
    "\n",
    "num_gpu = 2 \n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "latent_dim = 4 # type=int \"dimensionality of the latent space\"\n",
    "img_size = 1024 # type=int \"size of each image dimension\"\n",
    "channels = 1 # type=int \"number of image channels\"\n",
    "sample_interval = 10000 # int \"interval betwen image samples\"\n",
    "\n",
    "dataset_dir = r\"C:\\Users\\Leo's PC\\Documents\\SSTP Tests\\stylegan2-ada-pytorch\\Font1024\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, file_dir, transform=None):\n",
    "\n",
    "        self.dir = file_dir\n",
    "        self.transform = transform\n",
    "        self.diction = {}\n",
    "        \n",
    "        idx = 0\n",
    "        for filename in os.listdir(self.dir):\n",
    "            if filename.endswith('png'):\n",
    "                self.diction[idx] = filename\n",
    "                idx += 1\n",
    "                        \n",
    "    def __len__(self):\n",
    "        return len(self.diction) - 1\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.diction[idx], self.diction[idx + 1]\n",
    "        directory_x, directory_y = self.dir + \"\\\\\" + str(x), self.dir + \"\\\\\" + str(y)\n",
    "        x, y = cv.imread(directory_x, cv.IMREAD_GRAYSCALE), cv.imread(directory_y, cv.IMREAD_GRAYSCALE)\n",
    "        if self.transform:\n",
    "            x, y = self.transform(x), self.transform(y)\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "# dataset = Dataset(file_dir=dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-459f1d40f9fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        #expected inout size (N, 3, 512, 256)\n",
    "        \n",
    "        #=======\n",
    "        #Encoder\n",
    "        #=======\n",
    "        \n",
    "        self.conv1_1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=[3,3], stride=1, padding=1)\n",
    "        self.activation1_1 = nn.ReLU()\n",
    "        self.conv1_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=[3,3], stride=1, padding=1)\n",
    "        self.activation1_2 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size = [2, 2], stride=2, padding=0)\n",
    "        \n",
    "        self.conv2_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=[3,3], stride=1, padding=1)\n",
    "        self.activation2_1 = nn.ReLU()\n",
    "        self.conv2_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=[3,3], stride=1, padding=1)\n",
    "        self.activation2_2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = [2, 2], stride=2, padding=0)\n",
    "        \n",
    "        self.conv3_1 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=[3,3], stride=1, padding=1)\n",
    "        self.activation3_1 = nn.ReLU()\n",
    "        self.conv3_2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=[3,3], stride=2, padding=1)\n",
    "        self.activation3_2 = nn.ReLU()\n",
    "        self.conv3_3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=[3,3], stride=1, padding=1)\n",
    "        self.activation3_3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size = [2, 2], stride=2, padding=0)\n",
    "        \n",
    "        self.conv4_1 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=[3,3], stride=1, padding=1)\n",
    "        self.activation4_1 = nn.ReLU()\n",
    "        self.conv4_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=[3,3], stride=2, padding=1)\n",
    "        self.activation4_2 = nn.ReLU()\n",
    "        self.conv4_3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=[3,3], stride=1, padding=1)\n",
    "        self.activation4_3 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size = [2, 2], stride=2, padding=0)\n",
    "        \n",
    "        self.conv5_1 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=[3,3], stride=1, padding=1)\n",
    "        self.activation5_1 = nn.ReLU()\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size = [2, 2], stride=2, padding=0)\n",
    "        \n",
    "        self.conv5_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=[3,3], stride=2, padding=1)\n",
    "        self.activation5_2 = nn.ReLU()\n",
    "        self.conv5_3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=[3,3], stride=1, padding=1)\n",
    "        self.activation5_3 = nn.ReLU()\n",
    "        \n",
    "        #size = (N, 512, 4, 2)\n",
    "        \n",
    "        #=======\n",
    "        #Recurrent Module\n",
    "        #=======\n",
    "        \n",
    "        self.pool_r = nn.AdaptiveMaxPool2d(output_size = (2, 1))\n",
    "        \n",
    "        #size of GRU input = (batch_size, seq_len, inp_size)\n",
    "        self.gru = nn.GRU(input_size=1024, hidden_size=1024, num_layers=2, bias=True, batch_first=True, dropout=0.2)\n",
    "        \n",
    "        self.upsample_r = nn.Upsample(size=(8, 4))\n",
    "        \n",
    "        #=======\n",
    "        #Decoder\n",
    "        #=======\n",
    "        \n",
    "        #size = (N, 512, 8, 4)\n",
    "        \n",
    "        self.conv6_1 = nn.ConvTranspose2d(in_channels=512, out_channels=512, kernel_size=[3,3], stride=1, padding=1)\n",
    "        self.activation6_1 = nn.ReLU()\n",
    "        self.conv6_2 = nn.ConvTranspose2d(in_channels=512, out_channels=512, kernel_size=[3,3], stride=1, padding=1)\n",
    "        self.activation6_2 = nn.ReLU()\n",
    "        self.conv6_3 = nn.ConvTranspose2d(in_channels=512, out_channels=512, kernel_size=[3,3], stride=1, padding=1)\n",
    "        self.activation6_3 = nn.ReLU()\n",
    "        self.upsample6 = nn.Upsample(scale_factor=2)\n",
    "        \n",
    "        self.conv7_1 = nn.ConvTranspose2d(in_channels=512, out_channels=512, kernel_size=[3,3], stride=1, padding=1)\n",
    "        self.activation7_1 = nn.ReLU()\n",
    "        self.conv7_2 = nn.ConvTranspose2d(in_channels=512, out_channels=512, kernel_size=[3,3], stride=1, padding=1)\n",
    "        self.activation7_2 = nn.ReLU()\n",
    "        self.conv7_3 = nn.ConvTranspose2d(in_channels=512, out_channels=512, kernel_size=[3,3], stride=1, padding=1)\n",
    "        self.activation7_3 = nn.ReLU()\n",
    "        self.upsample7 = nn.Upsample(scale_factor=2)\n",
    "        \n",
    "        self.conv8_1 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=[3,3], stride=1, padding=1)\n",
    "        self.activation8_1 = nn.ReLU()\n",
    "        self.conv8_2 = nn.ConvTranspose2d(in_channels=256, out_channels=256, kernel_size=[3,3], stride=1, padding=1)\n",
    "        self.activation8_2 = nn.ReLU()\n",
    "        self.conv8_3 = nn.ConvTranspose2d(in_channels=256, out_channels=256, kernel_size=[3,3], stride=1, padding=1)\n",
    "        self.activation8_3 = nn.ReLU()\n",
    "        self.upsample8 = nn.Upsample(scale_factor=2)\n",
    "        \n",
    "        self.conv9_1 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=[3,3], stride=2, padding=1)\n",
    "        self.activation9_1 = nn.ReLU()\n",
    "        self.conv9_2 = nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=[3,3], stride=1, padding=1)\n",
    "        self.activation9_2 = nn.ReLU()\n",
    "        self.upsample9 = nn.Upsample(scale_factor=2)\n",
    "        \n",
    "        self.conv10_1 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=[3,3], stride=2, padding=1)\n",
    "        self.activation10_1 = nn.ReLU()\n",
    "        self.conv10_2 = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=[3,3], stride=1, padding=1)\n",
    "        self.activation10_2 = nn.ReLU()\n",
    "        \n",
    "        self.output = nn.ConvTranspose2d(in_channels=64, out_channels=3, kernel_size=[1,1], stride=1, padding=0)\n",
    "        self.activation_output = nn.Sigmoid()\n",
    "        \n",
    "        self.h = torch.Tensor(np.random.randn(2, 2, 1024)) #(num_layers, batchsize, inp.shape)\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        #=======\n",
    "        #Encoder\n",
    "        #=======\n",
    "        \n",
    "        x = self.conv1_1(x)\n",
    "        x = self.activation1_1(x)\n",
    "        x = self.conv1_2(x)\n",
    "        x = self.activation1_2(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2_1(x)\n",
    "        x = self.activation2_1(x)\n",
    "        x = self.conv2_2(x)\n",
    "        x = self.activation2_2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3_1(x)\n",
    "        x = self.activation3_1(x)\n",
    "        x = self.conv3_2(x)\n",
    "        x = self.activation3_2(x)\n",
    "        x = self.conv3_3(x)\n",
    "        x = self.activation3_3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = self.conv4_1(x)\n",
    "        x = self.activation4_1(x)\n",
    "        x = self.conv4_2(x)\n",
    "        x = self.activation4_2(x)\n",
    "        x = self.conv4_3(x)\n",
    "        x = self.activation4_3(x)\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        x = self.conv5_1(x)\n",
    "        x = self.activation5_1(x)\n",
    "        x = self.pool5(x)\n",
    "        \n",
    "        x = self.conv5_2(x)\n",
    "        x = self.activation5_2(x)\n",
    "        x = self.conv5_3(x)\n",
    "        x = self.activation5_3(x)\n",
    "        \n",
    "        #=======\n",
    "        #Recurrent Module\n",
    "        #=======\n",
    "        \n",
    "        print(\"before R pool\", x.shape)\n",
    "        \n",
    "        x = self.pool_r(x)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        x = self.fc_r_1(x)\n",
    "        \n",
    "        print('before gru', x.shape)\n",
    "        \n",
    "        x, self.h = self.gru(x, self.h)\n",
    "        \n",
    "        print('after gru', x.shape)\n",
    "        \n",
    "        x = x[:,-1]\n",
    "        \n",
    "        x = self.fc_r_2(x)\n",
    "        x = torch.reshape(x, (x.shape[0], 512, 4, 2))\n",
    "        \n",
    "        print('after reshape', x.shape)\n",
    "        \n",
    "        x = self.upsample_r(x)\n",
    "        \n",
    "        #=======\n",
    "        #Decoder\n",
    "        #=======\n",
    "        \n",
    "        x = self.conv6_1(x)\n",
    "        x = self.activation6_1(x)\n",
    "        x = self.conv6_2(x)\n",
    "        x = self.activation6_2(x)\n",
    "        x = self.conv6_3(x)\n",
    "        x = self.activation6_3(x)\n",
    "        x = self.upsample6(x)\n",
    "        \n",
    "        x = self.conv7_1(x)\n",
    "        x = self.activation7_1(x)\n",
    "        x = self.conv7_2(x)\n",
    "        x = self.activation7_2(x)\n",
    "        x = self.conv7_3(x)\n",
    "        x = self.activation7_3(x)\n",
    "        x = self.upsample7(x)\n",
    "        \n",
    "        x = self.conv8_1(x)\n",
    "        x = self.activation8_1(x)\n",
    "        x = self.conv8_2(x)\n",
    "        x = self.activation8_2(x)\n",
    "        x = self.conv8_3(x)\n",
    "        x = self.activation8_3(x)\n",
    "        x = self.upsample8(x)\n",
    "        \n",
    "        x = self.conv9_1(x)\n",
    "        x = self.activation9_1(x)\n",
    "        x = self.conv9_2(x)\n",
    "        x = self.activation9_2(x)\n",
    "        x = self.upsample9(x)\n",
    "        \n",
    "        x = self.conv10_1(x)\n",
    "        x = self.activation10_1(x)\n",
    "        x = self.conv10_2(x)\n",
    "        x = self.activation10_2(x)\n",
    "        \n",
    "        x = self.output(x)\n",
    "        x = self.activation_output(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def name(self):\n",
    "        return \"Generator\"\n",
    "    \n",
    "    \n",
    "    def load_SalGan_weights(self, pretrained_weights='gen_modelWeights0090.npz'):\n",
    "        \n",
    "        self.pretrained_weights = np.load(open(pretrained_weights, 'rb'), allow_pickle=True)\n",
    "        \n",
    "        self.layers = [module for module in self.modules() if type(module) != nn.Sequential]\n",
    "        self.conv_layers = [conv for conv in self.layers if type(conv) == nn.modules.conv.Conv2d or type(conv) == nn.modules.conv.ConvTranspose2d]\n",
    "\n",
    "        array_idx = 0\n",
    "        for layer in self.conv_layers:\n",
    "\n",
    "            if type(layer) == nn.modules.conv.ConvTranspose2d:\n",
    "                #the dim order of weight shape is messed up for ConvTranspose, so do transpose before loading\n",
    "\n",
    "                self.reshaped_array = self.pretrained_weights['arr_' + str(array_idx)].transpose(1, 0, 2, 3)\n",
    "\n",
    "                if layer.weight.shape == self.reshaped_array.shape:\n",
    "                    layer.weight = nn.Parameter(torch.from_numpy(self.reshaped_array).float())\n",
    "                else:\n",
    "                    print(\"Layer was initialized with Xavier due to shape mismatch\")\n",
    "                    torch.nn.init.xavier_uniform_(layer.weight)           \n",
    "                \n",
    "                array_idx += 1  #add 1 to idx to fetch the next element from the file\n",
    "\n",
    "                self.reshaped_array = self.pretrained_weights['arr_' + str(array_idx)]\n",
    "                \n",
    "                if layer.bias.shape == self.reshaped_array.shape:\n",
    "                    layer.bias = nn.Parameter(torch.from_numpy(self.reshaped_array.copy()).float())\n",
    "                else:\n",
    "                    layer.bias.data.fill_(0.01)\n",
    "\n",
    "            else: \n",
    "                layer.weight = nn.Parameter(torch.from_numpy(self.pretrained_weights['arr_' + str(array_idx)]).float())\n",
    "\n",
    "                array_idx += 1  #add 1 to idx to fetch the next element from the file\n",
    "\n",
    "                layer.bias = nn.Parameter(torch.from_numpy(self.pretrained_weights['arr_' + str(array_idx)]).float())\n",
    "\n",
    "            array_idx += 1 #add 1 to idx to fetch the next element from the file\n",
    "    \n",
    "    \n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.ResNet = torchvision.models.resnet34(pretrained=True)\n",
    "        self.ResNet.fc = nn.Linear(in_features=512, out_features=512, bias=True)\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=512, hidden_size=512, num_layers=3, bias=True, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Linear(512, 2, bias=True)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.h = torch.Tensor(np.random.randn(3, 2, 512))  #(num_layers, batchsize, inp.shape)\n",
    "\n",
    "    @autocast()\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.ResNet(x)\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        x, self.h = self.gru(x, self.h)\n",
    "        x = x[:,-1]\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def name(self):\n",
    "        return \"Discriminator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer was initialized with Xavier due to shape mismatch\n"
     ]
    }
   ],
   "source": [
    "G = Generator()\n",
    "D = Discriminator()\n",
    "G.load_SalGan_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before R pool torch.Size([2, 512, 2, 1])\n"
     ]
    },
    {
     "ename": "ModuleAttributeError",
     "evalue": "'Generator' object has no attribute 'fc_r_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-f39a083600b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mD_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-c3de90d37f52>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc_r_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'before gru'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    777\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m--> 779\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m    780\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleAttributeError\u001b[0m: 'Generator' object has no attribute 'fc_r_1'"
     ]
    }
   ],
   "source": [
    "inp = torch.Tensor(np.random.randn(2, 3, 512, 256))\n",
    "out = G(inp)\n",
    "print(out.shape)\n",
    "D_out = D(out)\n",
    "print(D_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss, Optimizer, Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "G = Generator()\n",
    "D = Discriminator()\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d or type(m) == nn.ConvTranspose2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "G.apply(init_weights)\n",
    "D.apply(init_weights)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "G.cuda()\n",
    "D.cuda()\n",
    "adversarial_loss.cuda()\n",
    "\n",
    "\n",
    "G = torch.nn.DataParallel(G)\n",
    "D = torch.nn.DataParallel(D)\n",
    "\n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\pd\\lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 3563/3564] [D loss: 0.724080] [G loss: 0.474079]\n",
      "[Epoch 1/100] [Batch 3563/3564] [D loss: 0.724085] [G loss: 0.474081]\n",
      "[Epoch 2/100] [Batch 3563/3564] [D loss: 0.724079] [G loss: 0.474079]\n",
      "[Epoch 3/100] [Batch 3563/3564] [D loss: 0.724083] [G loss: 0.474081]\n",
      "[Epoch 4/100] [Batch 3563/3564] [D loss: 0.724084] [G loss: 0.474086]\n",
      "[Epoch 5/100] [Batch 3563/3564] [D loss: 0.724084] [G loss: 0.474077]\n",
      "[Epoch 6/100] [Batch 3563/3564] [D loss: 0.724077] [G loss: 0.474077]\n"
     ]
    }
   ],
   "source": [
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    for idx, imgs in enumerate(loader):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(imgs.shape[0], 2).fill_(1.0), requires_grad=False).cuda()\n",
    "        fake = Variable(Tensor(imgs.shape[0], 2).fill_(0.0), requires_grad=False).cuda()\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor)).cuda().to(device)\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        latent_vector = Variable(Tensor(np.random.randn(imgs.shape[0], 512, latent_dim, latent_dim))).cuda()\n",
    "        \n",
    "        G.train()\n",
    "        D.eval()\n",
    "        \n",
    "        with autocast():\n",
    "            gen_imgs = G(latent_vector) # Generate a batch of images\n",
    "            g_loss = adversarial_loss(D(gen_imgs), valid) # Loss measures generator's ability to fool the discriminator\n",
    "\n",
    "        scaler.scale(g_loss).backward() #back propagation with calculated loss\n",
    "        scaler.step(optimizer_G) \n",
    "        scaler.update()\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        \n",
    "        D.train()\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        real_imgs.unsqueeze_(1)\n",
    "        \n",
    "        with autocast():\n",
    "            # Measure discriminator's ability to classify real from generated samples\n",
    "            real_loss = adversarial_loss(D(real_imgs), valid)\n",
    "            fake_loss = adversarial_loss(D(gen_imgs.detach()), fake)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        scaler.scale(d_loss).backward() #back propagation with calculated loss\n",
    "        scaler.step(optimizer_D) \n",
    "        scaler.update()\n",
    "\n",
    "       \n",
    "        batches_done = epoch * len(loader) + idx\n",
    "        \n",
    "        \n",
    "    save_image(gen_imgs.data[:25], r\"C:/Users/Leo's PC/Documents/SSTP Tests/Chinese Characters/LightGAN out/%d.png\" % batches_done, nrow=5, normalize=True)\n",
    "    print(\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\" % (epoch, n_epochs, idx, len(loader), d_loss.item(), g_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "checkpoint_file = open(r\"C:/Users/Leo's PC/Documents/SSTP Tests/Chinese Characters/LightGAN out/G.tar\", 'wb')\n",
    "torch.save({'model': G.state_dict()}, checkpoint_file)\n",
    "checkpoint_file.close()\n",
    "\n",
    "checkpoint = torch.load(open(\"C:/Users/Leo's PC/Documents/SSTP Tests/Chinese Characters/LightGAN out/G.tar\", 'rb'))\n",
    "G.load_state_dict(checkpoint['model'])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
